{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Based Segmentation of Fundus Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image names\n",
    "import os\n",
    "import os.path\n",
    "import cv2\n",
    "\n",
    "from sklearn.datasets import load_sample_image\n",
    "from sklearn.feature_extraction import image as imgutil\n",
    "\n",
    "import time\n",
    "import torch.utils.data as utils\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)                         # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))    # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.exp()\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * at\n",
    "\n",
    "        loss = -1 * (1 - pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassClassifier, self).__init__()\n",
    "        self.conv = nn.Sequential()\n",
    "        self.conv.add_module(\"Pad1\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv1\", nn.Conv2d(3, 32, kernel_size=2))\n",
    "        self.conv.add_module(\"BN1\", nn.BatchNorm2d(32))\n",
    "        self.conv.add_module(\"Relu1\", nn.ReLU())\n",
    "        \n",
    "        self.conv.add_module(\"Pad2\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv2\", nn.Conv2d(32, 32, kernel_size=2))\n",
    "        self.conv.add_module(\"BN2\", nn.BatchNorm2d(32))\n",
    "        self.conv.add_module(\"Relu2\", nn.ReLU())\n",
    "        self.conv.add_module(\"Layer2MaxPool\", nn.MaxPool2d(2))\n",
    "        \n",
    "        self.conv.add_module(\"Pad3\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv3\", nn.Conv2d(32, 64, kernel_size=2))\n",
    "        self.conv.add_module(\"BN3\", nn.BatchNorm2d(64))\n",
    "        self.conv.add_module(\"Relu3\", nn.ReLU())\n",
    "        \n",
    "        self.conv.add_module(\"Pad4\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv4\", nn.Conv2d(64, 64, kernel_size=2))\n",
    "        self.conv.add_module(\"BN4\", nn.BatchNorm2d(64))\n",
    "        self.conv.add_module(\"Relu4\", nn.ReLU())\n",
    "        self.conv.add_module(\"Layer4MaxPool\", nn.MaxPool2d(2))\n",
    "                             \n",
    "        self.conv.add_module(\"Pad5\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv5\", nn.Conv2d(64, 128, kernel_size=2))\n",
    "        self.conv.add_module(\"BN5\", nn.BatchNorm2d(128))\n",
    "        self.conv.add_module(\"Relu5\", nn.ReLU())\n",
    "        \n",
    "        self.conv.add_module(\"Pad6\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv6\", nn.Conv2d(128, 128, kernel_size=2))\n",
    "        self.conv.add_module(\"BN6\", nn.BatchNorm2d(128))\n",
    "        self.conv.add_module(\"Relu6\", nn.ReLU())\n",
    "        self.conv.add_module(\"Layer6MaxPool\", nn.MaxPool2d(2))\n",
    "        \n",
    "        self.conv.add_module(\"Pad7\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv7\", nn.Conv2d(128, 256, kernel_size=2))\n",
    "        self.conv.add_module(\"BN7\", nn.BatchNorm2d(256))\n",
    "        self.conv.add_module(\"Relu7\", nn.ReLU())\n",
    "        \n",
    "        self.conv.add_module(\"Pad8\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv8\", nn.Conv2d(256, 256, kernel_size=2))\n",
    "        self.conv.add_module(\"BN8\", nn.BatchNorm2d(256))\n",
    "        self.conv.add_module(\"Relu8\", nn.ReLU())\n",
    "        \n",
    "        self.fc = nn.Sequential()\n",
    "        self.fc.add_module(\"FC1\", nn.Linear(4096, 1000))\n",
    "        self.fc.add_module(\"Relu9\", nn.ReLU())\n",
    "        self.fc.add_module(\"Dropout1\", nn.Dropout(0.5))\n",
    "        self.fc.add_module(\"FC2\", nn.Linear(1000, 100))\n",
    "        self.fc.add_module(\"Relu10\", nn.ReLU())\n",
    "        self.fc.add_module(\"Dropout1\", nn.Dropout(0.5))\n",
    "        self.fc.add_module(\"FC3\",nn.Linear(100, 5)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.conv = nn.Sequential()\n",
    "        self.conv.add_module(\"Pad1\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv1\", nn.Conv2d(3, 32, kernel_size=2))\n",
    "        self.conv.add_module(\"BN1\", nn.BatchNorm2d(32))\n",
    "        self.conv.add_module(\"Relu1\", nn.ReLU())\n",
    "        \n",
    "        self.conv.add_module(\"Pad2\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv2\", nn.Conv2d(32, 32, kernel_size=2))\n",
    "        self.conv.add_module(\"BN2\", nn.BatchNorm2d(32))\n",
    "        self.conv.add_module(\"Relu2\", nn.ReLU())\n",
    "        self.conv.add_module(\"Layer2MaxPool\", nn.MaxPool2d(2))\n",
    "        \n",
    "        self.conv.add_module(\"Pad3\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv3\", nn.Conv2d(32, 64, kernel_size=2))\n",
    "        self.conv.add_module(\"BN3\", nn.BatchNorm2d(64))\n",
    "        self.conv.add_module(\"Relu3\", nn.ReLU())\n",
    "        \n",
    "        self.conv.add_module(\"Pad4\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv4\", nn.Conv2d(64, 64, kernel_size=2))\n",
    "        self.conv.add_module(\"BN4\", nn.BatchNorm2d(64))\n",
    "        self.conv.add_module(\"Relu4\", nn.ReLU())\n",
    "        self.conv.add_module(\"Layer4MaxPool\", nn.MaxPool2d(2))\n",
    "                             \n",
    "        self.conv.add_module(\"Pad5\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv5\", nn.Conv2d(64, 128, kernel_size=2))\n",
    "        self.conv.add_module(\"BN5\", nn.BatchNorm2d(128))\n",
    "        self.conv.add_module(\"Relu5\", nn.ReLU())\n",
    "        \n",
    "        self.conv.add_module(\"Pad6\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv6\", nn.Conv2d(128, 128, kernel_size=2))\n",
    "        self.conv.add_module(\"BN6\", nn.BatchNorm2d(128))\n",
    "        self.conv.add_module(\"Relu6\", nn.ReLU())\n",
    "        self.conv.add_module(\"Layer6MaxPool\", nn.MaxPool2d(2))\n",
    "        \n",
    "        self.conv.add_module(\"Pad7\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv7\", nn.Conv2d(128, 256, kernel_size=2))\n",
    "        self.conv.add_module(\"BN7\", nn.BatchNorm2d(256))\n",
    "        self.conv.add_module(\"Relu7\", nn.ReLU())\n",
    "        \n",
    "        self.conv.add_module(\"Pad8\", nn.ConstantPad2d((0,1,0,1), 0))\n",
    "        self.conv.add_module(\"Conv8\", nn.Conv2d(256, 256, kernel_size=2))\n",
    "        self.conv.add_module(\"BN8\", nn.BatchNorm2d(256))\n",
    "        self.conv.add_module(\"Relu8\", nn.ReLU())\n",
    "        \n",
    "        self.fc = nn.Sequential()\n",
    "        self.fc.add_module(\"FC1\", nn.Linear(4096, 1000))\n",
    "        self.fc.add_module(\"Relu9\", nn.ReLU())\n",
    "        self.fc.add_module(\"Dropout1\", nn.Dropout(0.5))\n",
    "        self.fc.add_module(\"FC2\", nn.Linear(1000, 100))\n",
    "        self.fc.add_module(\"Relu10\", nn.ReLU())\n",
    "        self.fc.add_module(\"Dropout1\", nn.Dropout(0.5))\n",
    "        self.fc.add_module(\"FC3\",nn.Linear(100, 2)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MultiClassClassifier()\n",
    "model = BinaryClassifier()\n",
    "model.cuda()\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================\n",
    "# We may wish to load a previous model\n",
    "load_saved = False\n",
    "if load_saved:\n",
    "    saved_models = list(map(lambda x: os.path.splitext(x)[0], filter(lambda x: os.path.splitext(x)[1] == '.sav', os.listdir('.'))))\n",
    "    print(saved_models)\n",
    "    attempts = 0\n",
    "    while attempts < 3:\n",
    "        attempts += 1\n",
    "        model_name = input(\"Choose a saved model: \")\n",
    "        if model_name not in saved_models:\n",
    "            print(\"Not found. Try again\")\n",
    "            continue\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(model_name+\".sav\"))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labelled_patches(img, truth):\n",
    "    img = np.dstack((img, truth))\n",
    "    patches = imgutil.extract_patches_2d(img, (32,32))\n",
    "    np.random.shuffle(patches)\n",
    "    positive_patches = []\n",
    "    negative_patches = []\n",
    "    positive_labels = []\n",
    "    for patch in patches:\n",
    "              \n",
    "        truth = patch[16,16,3]\n",
    "        patch = patch[:,:,:3]  \n",
    "        patch = np.rollaxis(patch,2,0)\n",
    "\n",
    "        if truth != 0:\n",
    "            positive_patches.append(torch.from_numpy(patch))\n",
    "            positive_labels.append(truth)\n",
    "        else:\n",
    "            negative_patches.append(torch.from_numpy(patch))\n",
    "    \n",
    "    if len(positive_patches) == 0:\n",
    "        positive_tensor = None\n",
    "    else:\n",
    "        positive_tensor = torch.stack(positive_patches)\n",
    "    \n",
    "    return positive_tensor, torch.stack(negative_patches), positive_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensors(images, features):\n",
    "    \n",
    "    patches = []\n",
    "    labels = []\n",
    "    \n",
    "    for image, feature in zip(images, features):\n",
    "        pos, neg, pos_labels = get_labelled_patches(image, feature)\n",
    "        if(pos is None):\n",
    "            continue\n",
    "        neg = neg[:len(pos),:,:]\n",
    "        patches.extend(pos)\n",
    "        patches.extend(neg)\n",
    "            \n",
    "        pos_l = torch.Tensor(pos_labels)\n",
    "        neg_l = torch.zeros(len(neg))\n",
    "        labels.extend(pos_l)\n",
    "        labels.extend(neg_l)\n",
    "    \n",
    "    image_tensor = torch.stack(patches).float()\n",
    "    label_tensor = torch.stack(labels)\n",
    "    return image_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_numpy_data_task1(source):\n",
    "\n",
    "    # train_path_truths = os.path.join(\"Data_Group_Component_Task_1\", \"Train\", \"masks_Hard_Exudates\")\n",
    "    train_path_images = os.path.join(\"Data_Group_Component_Task_1\", source, \"original_retinal_images\")\n",
    "\n",
    "    train_path_exudates = os.path.join(\"Data_Group_Component_Task_1\", source, \"masks_Hard_Exudates\")\n",
    "    train_path_soft_exudates = os.path.join(\"Data_Group_Component_Task_1\", source, \"masks_Soft_Exudates\")\n",
    "    train_path_haemorrhages = os.path.join(\"Data_Group_Component_Task_1\", source, \"masks_Haemorrhages\")\n",
    "    train_path_microaneurysms = os.path.join(\"Data_Group_Component_Task_1\", source, \"masks_Microaneurysms\")\n",
    "\n",
    "    train_image_names = os.listdir(train_path_images)\n",
    "\n",
    "    train_exudate_names = list(map(lambda x: os.path.join(train_path_exudates, x.split('.')[0] + '_EX.tif'), train_image_names))\n",
    "    train_haem_names = list(map(lambda x: os.path.join(train_path_haemorrhages, x.split('.')[0] + '_HE.tif'), train_image_names))\n",
    "    train_sfex_names = list(map(lambda x: os.path.join(train_path_soft_exudates, x.split('.')[0] + '_SE.tif'), train_image_names))\n",
    "    train_ma_names = list(map(lambda x: os.path.join(train_path_microaneurysms, x.split('.')[0] + '_MA.tif'), train_image_names))\n",
    "\n",
    "    images = list(map(\n",
    "        lambda x: cv2.resize(\n",
    "            cv2.imread(\n",
    "                os.path.join(train_path_images, x)\n",
    "            ), (256, 256)\n",
    "        ), train_image_names))\n",
    "\n",
    "    \n",
    "    features = []\n",
    "    for names in zip(train_exudate_names, train_haem_names, train_sfex_names, train_ma_names):\n",
    "        he, ha, se, ma =  names\n",
    "\n",
    "        he = cv2.imread(he)\n",
    "        if he is not None:\n",
    "            he = cv2.resize(he, (256,256))[:,:,2]\n",
    "        else:\n",
    "            he = np.zeros((256,256), dtype=np.uint8)\n",
    "\n",
    "        ha = cv2.imread(ha)\n",
    "        if ha is not None:\n",
    "            ha = cv2.resize(ha, (256,256))[:,:,2]\n",
    "        else:\n",
    "            ha = np.zeros((256,256), dtype=np.uint8)\n",
    "\n",
    "        se = cv2.imread(se)\n",
    "        if se is not None:\n",
    "            se = cv2.resize(se, (256,256))[:,:,2]\n",
    "        else:\n",
    "            se = np.zeros((256,256), dtype=np.uint8)\n",
    "\n",
    "        ma = cv2.imread(ma)\n",
    "        if ma is not None:\n",
    "            ma = cv2.resize(ma, (256,256))[:,:,2]\n",
    "        else:\n",
    "            ma = np.zeros((256,256), dtype=np.uint8)    \n",
    "\n",
    "        feature_map = (he != 0).astype(np.uint8)\n",
    "        feature_map[np.where(ha != 0)] = 2\n",
    "        feature_map[np.where(se != 0)] = 3\n",
    "        feature_map[np.where(ma != 0)] = 4\n",
    "\n",
    "        features.append(feature_map)\n",
    "    \n",
    "    return images, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_numpy_data_task2(source):\n",
    "    \n",
    "    def read_video(video_path):\n",
    "        video = cv2.VideoCapture(str(video_path))\n",
    "        while video.isOpened():\n",
    "            ok, frame = video.read()\n",
    "\n",
    "            if not ok:\n",
    "                break\n",
    "\n",
    "            yield frame\n",
    "        video.release()\n",
    "\n",
    "    if source == \"Train\":\n",
    "        source = \"Training\"\n",
    "    # train_path_truths = os.path.join(\"Data_Group_Component_Task_1\", \"Train\", \"masks_Hard_Exudates\")\n",
    "    train_path_images = os.path.join(\"Data_Group_Component_Task_2\", source, \"original_retinal_images\")\n",
    "\n",
    "    train_path_vessels = os.path.join(\"Data_Group_Component_Task_2\", source, \"blood_vessel_segmentation_masks\")\n",
    "   \n",
    "    train_image_names = os.listdir(train_path_images)\n",
    "\n",
    "    train_vessel_names = list(map(lambda x: os.path.join(train_path_vessels, x.split('_')[0] + '_manual1.gif'), train_image_names))\n",
    "\n",
    "    images = list(map(\n",
    "        lambda x: cv2.resize(\n",
    "            cv2.imread(\n",
    "                os.path.join(train_path_images, x)\n",
    "            ), (256, 256)\n",
    "        ), train_image_names))\n",
    "    \n",
    "    features = []\n",
    "    for name in train_vessel_names:\n",
    "        \n",
    "        vessels = list(read_video(name))[0]\n",
    "        \n",
    "        if vessels is not None:\n",
    "            vessels = cv2.resize(vessels, (256,256))[:,:,2]\n",
    "        else:\n",
    "            vessels = np.zeros((256,256), dtype=np.uint8)\n",
    "\n",
    "        feature_map = (vessels != 0).astype(np.uint8)\n",
    "\n",
    "        features.append(feature_map)\n",
    "    \n",
    "    return images, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_numpy_data_task1_specificfeature(source, feature, suffix):\n",
    "    \n",
    "    # train_path_truths = os.path.join(\"Data_Group_Component_Task_1\", \"Train\", \"masks_Hard_Exudates\")\n",
    "    train_path_images = os.path.join(\"Data_Group_Component_Task_1\", source, \"original_retinal_images\")\n",
    "\n",
    "    train_path_vessels = os.path.join(\"Data_Group_Component_Task_1\", source, feature)\n",
    "   \n",
    "    train_image_names = os.listdir(train_path_images)\n",
    "\n",
    "    train_feature_names = list(map(lambda x: os.path.join(train_path_vessels, x.split('.')[0] + f'_{suffix}.tif'), train_image_names))\n",
    "\n",
    "    images = list(map(\n",
    "        lambda x: cv2.resize(\n",
    "            cv2.imread(\n",
    "                os.path.join(train_path_images, x)\n",
    "            ), (256, 256)\n",
    "        ), train_image_names))\n",
    "    \n",
    "    features = []\n",
    "    for name in train_feature_names:\n",
    "    \n",
    "        vessels = cv2.imread(name)\n",
    "\n",
    "        \n",
    "        if vessels is not None:\n",
    "            vessels = cv2.resize(vessels, (256,256))[:,:,2]\n",
    "        else:\n",
    "            vessels = np.zeros((256,256), dtype=np.uint8)\n",
    "\n",
    "        feature_map = (vessels != 0).astype(np.uint8)\n",
    "\n",
    "        features.append(feature_map)\n",
    "    \n",
    "    return images, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the correct data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list, label_list = load_numpy_data_task1_specificfeature(\"Train\", \"masks_Soft_Exudates\", \"SE\")\n",
    "# image_list, label_list = load_numpy_data_task2(\"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_tensor, label_tensor = create_tensors(image_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the data into pytorch DataLoader for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11566\n"
     ]
    }
   ],
   "source": [
    "print(len(image_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partitions(n, parts):\n",
    "    \n",
    "    a = np.linspace(0, n, parts+1).astype(np.uint64)\n",
    "\n",
    "    prev = a[0]\n",
    "    partitions=[]\n",
    "    for index in a[1:]:\n",
    "        partitions.append((prev, index))\n",
    "        prev=index\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2313), (2313, 4626), (4626, 6939), (6939, 9252), (9252, 11566)]\n"
     ]
    }
   ],
   "source": [
    "partitions = create_partitions(len(image_tensor), 5)\n",
    "print(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================  Partition:1  ======================================\n",
      "Epoch: 1 Loss:7.7166856955736876 Trained in 1.4014768600463867 seconds\n",
      "Epoch: 2 Loss:1.1241815702960594 Trained in 1.4092507362365723 seconds\n",
      "Epoch: 3 Loss:0.10534545283007901 Trained in 1.389035940170288 seconds\n",
      "Epoch: 4 Loss:0.01636983340995357 Trained in 1.4002623558044434 seconds\n",
      "Epoch: 5 Loss:0.007772507643238669 Trained in 1.4910125732421875 seconds\n",
      "Epoch: 6 Loss:0.00923547380313039 Trained in 1.5627751350402832 seconds\n",
      "Epoch: 7 Loss:0.005448279186399674 Trained in 1.5048158168792725 seconds\n",
      "Epoch: 8 Loss:0.11509303577413021 Trained in 1.4581074714660645 seconds\n",
      "Epoch: 9 Loss:0.226950641502647 Trained in 1.5351450443267822 seconds\n",
      "Epoch: 10 Loss:0.9191907774568904 Trained in 1.765279769897461 seconds\n",
      "Epoch: 11 Loss:0.02978351098136045 Trained in 1.6191380023956299 seconds\n",
      "Epoch: 12 Loss:0.01484833801532659 Trained in 1.5114150047302246 seconds\n",
      "Epoch: 13 Loss:0.00535729956789055 Trained in 1.701448917388916 seconds\n",
      "Epoch: 14 Loss:0.0082472923036363 Trained in 1.4451539516448975 seconds\n",
      "Epoch: 15 Loss:0.0042646546110063355 Trained in 1.389317512512207 seconds\n",
      "Epoch: 16 Loss:0.001859588441931237 Trained in 1.4134440422058105 seconds\n",
      "Epoch: 17 Loss:0.0016362612117859499 Trained in 1.870995044708252 seconds\n",
      "Epoch: 18 Loss:0.0033083605624852908 Trained in 1.6465950012207031 seconds\n",
      "Epoch: 19 Loss:0.0018428391491056573 Trained in 2.063479423522949 seconds\n",
      "Epoch: 20 Loss:0.0012158997202647015 Trained in 1.6156775951385498 seconds\n",
      "Epoch: 21 Loss:0.0013893916181828558 Trained in 1.7792408466339111 seconds\n",
      "Epoch: 22 Loss:0.001100087895000712 Trained in 1.732365608215332 seconds\n",
      "Epoch: 23 Loss:0.003127969184951951 Trained in 1.4549987316131592 seconds\n",
      "Epoch: 24 Loss:0.0011921930232077216 Trained in 1.5249223709106445 seconds\n",
      "Epoch: 25 Loss:0.0014724788532607391 Trained in 1.4471285343170166 seconds\n",
      "Epoch: 26 Loss:0.0032208694071425015 Trained in 1.4541089534759521 seconds\n",
      "Epoch: 27 Loss:0.10597815872002059 Trained in 1.4920077323913574 seconds\n",
      "Epoch: 28 Loss:0.9538585910186157 Trained in 1.4391515254974365 seconds\n",
      "Epoch: 29 Loss:0.19860574206904857 Trained in 1.4716145992279053 seconds\n",
      "Epoch: 30 Loss:0.03607512590360784 Trained in 1.401832103729248 seconds\n",
      "Epoch: 31 Loss:0.010094192070937424 Trained in 1.431171178817749 seconds\n",
      "Epoch: 32 Loss:0.006902490353240864 Trained in 1.390282154083252 seconds\n",
      "Epoch: 33 Loss:0.013427990839886661 Trained in 1.4281790256500244 seconds\n",
      "Epoch: 34 Loss:0.007502317969738215 Trained in 1.4630851745605469 seconds\n",
      "Epoch: 35 Loss:0.003147102117223355 Trained in 1.4082326889038086 seconds\n",
      "Epoch: 36 Loss:0.0020198530625634703 Trained in 1.4152154922485352 seconds\n",
      "Epoch: 37 Loss:0.006327899982011331 Trained in 1.477048635482788 seconds\n",
      "Epoch: 38 Loss:0.0009604336746207309 Trained in 1.4341835975646973 seconds\n",
      "Epoch: 39 Loss:0.0030341408928649116 Trained in 1.3925156593322754 seconds\n",
      "Epoch: 40 Loss:0.0008539216318865783 Trained in 1.3259170055389404 seconds\n",
      "Epoch: 41 Loss:0.00136834255565077 Trained in 1.377072811126709 seconds\n",
      "Epoch: 42 Loss:0.0011557353569973117 Trained in 1.349677562713623 seconds\n",
      "Epoch: 43 Loss:0.0005077362025929233 Trained in 1.3545453548431396 seconds\n",
      "Epoch: 44 Loss:0.0017281696955624426 Trained in 1.3483929634094238 seconds\n",
      "Epoch: 45 Loss:0.00032079073059065877 Trained in 1.3404154777526855 seconds\n",
      "Epoch: 46 Loss:0.0005169208248219093 Trained in 1.383298397064209 seconds\n",
      "Epoch: 47 Loss:0.0004885284719584604 Trained in 1.3599038124084473 seconds\n",
      "Epoch: 48 Loss:0.0001857537488731964 Trained in 1.370331048965454 seconds\n",
      "Epoch: 49 Loss:0.000506499365533486 Trained in 1.3503870964050293 seconds\n",
      "Epoch: 50 Loss:0.0002665622436026638 Trained in 1.3394172191619873 seconds\n",
      "Epoch: 51 Loss:0.0003542947791483897 Trained in 1.3300323486328125 seconds\n",
      "Epoch: 52 Loss:0.0006831477285320631 Trained in 1.3538646697998047 seconds\n",
      "Epoch: 53 Loss:0.0005523261766029464 Trained in 1.3992576599121094 seconds\n",
      "Epoch: 54 Loss:0.0004196629162205312 Trained in 1.3364465236663818 seconds\n",
      "Epoch: 55 Loss:0.00048802449983398333 Trained in 1.3483922481536865 seconds\n",
      "Epoch: 56 Loss:0.0003615188619416898 Trained in 1.3249354362487793 seconds\n",
      "Epoch: 57 Loss:0.0010456261489455443 Trained in 1.3618321418762207 seconds\n",
      "Epoch: 58 Loss:0.0002366256741126449 Trained in 1.3730361461639404 seconds\n",
      "Epoch: 59 Loss:0.00017284650137661117 Trained in 1.3662524223327637 seconds\n",
      "Epoch: 60 Loss:0.00014914659450582235 Trained in 1.3284637928009033 seconds\n",
      "Epoch: 61 Loss:0.0001276027223386933 Trained in 1.354377269744873 seconds\n",
      "Epoch: 62 Loss:0.0007510518994386928 Trained in 1.359363317489624 seconds\n",
      "Epoch: 63 Loss:0.0004470586749860672 Trained in 1.3476605415344238 seconds\n",
      "Epoch: 64 Loss:0.004480825604312955 Trained in 1.341832160949707 seconds\n",
      "Epoch: 65 Loss:0.0005080813622804214 Trained in 1.3445062637329102 seconds\n",
      "Epoch: 66 Loss:0.00033381975999802194 Trained in 1.361255407333374 seconds\n",
      "Epoch: 67 Loss:0.00031578670140852694 Trained in 1.357240915298462 seconds\n",
      "Epoch: 68 Loss:0.00019085297305565518 Trained in 1.3344528675079346 seconds\n",
      "Epoch: 69 Loss:0.007152136408729248 Trained in 1.3304615020751953 seconds\n",
      "Epoch: 70 Loss:0.00010295721135200608 Trained in 1.3623759746551514 seconds\n",
      "Epoch: 71 Loss:0.00019110679596678892 Trained in 1.3653011322021484 seconds\n",
      "Epoch: 72 Loss:0.003117235792737816 Trained in 1.34498929977417 seconds\n",
      "Epoch: 73 Loss:0.00013887588834471387 Trained in 1.3413593769073486 seconds\n",
      "Epoch: 74 Loss:0.00013246683147905003 Trained in 1.3195900917053223 seconds\n",
      "Epoch: 75 Loss:7.531642836333674e-05 Trained in 1.3655483722686768 seconds\n",
      "Epoch: 76 Loss:3.631445095564345e-05 Trained in 1.3186111450195312 seconds\n",
      "Epoch: 77 Loss:5.3428137238853424e-05 Trained in 1.3353958129882812 seconds\n",
      "Epoch: 78 Loss:8.565462462861717e-05 Trained in 1.362335443496704 seconds\n",
      "Epoch: 79 Loss:3.8657188278889976e-05 Trained in 1.3463988304138184 seconds\n",
      "Epoch: 80 Loss:8.723662407206234e-05 Trained in 1.3434064388275146 seconds\n",
      "Epoch: 81 Loss:2.481460576753136e-05 Trained in 1.342362642288208 seconds\n",
      "Epoch: 82 Loss:9.264616030435491e-05 Trained in 1.3306727409362793 seconds\n",
      "Epoch: 83 Loss:5.986213681197228e-05 Trained in 1.3295540809631348 seconds\n",
      "Epoch: 84 Loss:5.4615460927109893e-05 Trained in 1.353379487991333 seconds\n",
      "Epoch: 85 Loss:3.616809868134396e-05 Trained in 1.3281970024108887 seconds\n",
      "Epoch: 86 Loss:3.4857163027979254e-05 Trained in 1.3434162139892578 seconds\n",
      "Epoch: 87 Loss:7.204055770060336e-05 Trained in 1.3832995891571045 seconds\n",
      "Epoch: 88 Loss:1.4671545729427748e-05 Trained in 1.376619815826416 seconds\n",
      "Epoch: 89 Loss:1.8233152246693862e-05 Trained in 1.326472282409668 seconds\n",
      "Epoch: 90 Loss:2.4895667888102935e-05 Trained in 1.3496360778808594 seconds\n",
      "Epoch: 91 Loss:3.8544581959953916e-05 Trained in 1.312962293624878 seconds\n",
      "Epoch: 92 Loss:1.427650465224417e-05 Trained in 1.353379487991333 seconds\n",
      "Epoch: 93 Loss:2.1286010625232166e-05 Trained in 1.3480267524719238 seconds\n",
      "Epoch: 94 Loss:5.346298200059607e-05 Trained in 1.3369765281677246 seconds\n",
      "Epoch: 95 Loss:0.0002164936140633955 Trained in 1.3310046195983887 seconds\n",
      "Epoch: 96 Loss:1.206581400836626e-05 Trained in 1.325488567352295 seconds\n",
      "Epoch: 97 Loss:6.129741783489351e-05 Trained in 1.3458032608032227 seconds\n",
      "Epoch: 98 Loss:0.00019894123430930222 Trained in 1.3513514995574951 seconds\n",
      "Epoch: 99 Loss:1.5759467865805732e-05 Trained in 1.3422226905822754 seconds\n",
      "Epoch: 100 Loss:9.045601059298747e-06 Trained in 1.34340500831604 seconds\n",
      "================================  Partition:2  ======================================\n",
      "Epoch: 1 Loss:3.7384861055434158 Trained in 1.3461763858795166 seconds\n",
      "Epoch: 2 Loss:0.3826080959479441 Trained in 1.3331272602081299 seconds\n",
      "Epoch: 3 Loss:0.06813081319620551 Trained in 1.3234243392944336 seconds\n",
      "Epoch: 4 Loss:0.019173615431554936 Trained in 1.3316056728363037 seconds\n",
      "Epoch: 5 Loss:0.010888762611614311 Trained in 1.3432135581970215 seconds\n",
      "Epoch: 6 Loss:0.010943465516913164 Trained in 1.3487110137939453 seconds\n",
      "Epoch: 7 Loss:0.0019373053572095955 Trained in 1.3602650165557861 seconds\n",
      "Epoch: 8 Loss:0.010755483044841085 Trained in 1.371894359588623 seconds\n",
      "Epoch: 9 Loss:0.498906453861089 Trained in 1.3551831245422363 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Loss:0.8959252558231583 Trained in 1.3384056091308594 seconds\n",
      "Epoch: 11 Loss:0.08052442825794515 Trained in 1.349423885345459 seconds\n",
      "Epoch: 12 Loss:0.043655478921664326 Trained in 1.3446543216705322 seconds\n",
      "Epoch: 13 Loss:0.01632654967875169 Trained in 1.3538165092468262 seconds\n",
      "Epoch: 14 Loss:0.060084884878051525 Trained in 1.3487300872802734 seconds\n",
      "Epoch: 15 Loss:0.00734053549568614 Trained in 1.3563926219940186 seconds\n",
      "Epoch: 16 Loss:0.006741037180944431 Trained in 1.3494219779968262 seconds\n",
      "Epoch: 17 Loss:0.002902398294015285 Trained in 1.336864709854126 seconds\n",
      "Epoch: 18 Loss:0.00369037296855268 Trained in 1.3430864810943604 seconds\n",
      "Epoch: 19 Loss:0.0032257813765710353 Trained in 1.3493919372558594 seconds\n",
      "Epoch: 20 Loss:0.001947049552533997 Trained in 1.359678030014038 seconds\n",
      "Epoch: 21 Loss:0.0010511178159013923 Trained in 1.3593904972076416 seconds\n",
      "Epoch: 22 Loss:0.0035668626154858885 Trained in 1.341423749923706 seconds\n",
      "Epoch: 23 Loss:0.0013697589681136435 Trained in 1.3467161655426025 seconds\n",
      "Epoch: 24 Loss:0.001923804310848709 Trained in 1.3244569301605225 seconds\n",
      "Epoch: 25 Loss:0.0007957498801509644 Trained in 1.333423376083374 seconds\n",
      "Epoch: 26 Loss:0.0039644886016585446 Trained in 1.473060131072998 seconds\n",
      "Epoch: 27 Loss:0.0002777015195469801 Trained in 1.5239505767822266 seconds\n",
      "Epoch: 28 Loss:0.00042222499953936676 Trained in 1.3468539714813232 seconds\n",
      "Epoch: 29 Loss:0.000306941909174796 Trained in 1.3513848781585693 seconds\n",
      "Epoch: 30 Loss:0.000589873044734901 Trained in 1.3613579273223877 seconds\n",
      "Epoch: 31 Loss:0.0009950637744147173 Trained in 1.3402776718139648 seconds\n",
      "Epoch: 32 Loss:0.00032017707381370997 Trained in 1.353003978729248 seconds\n",
      "Epoch: 33 Loss:0.00017151355826072745 Trained in 1.3364243507385254 seconds\n",
      "Epoch: 34 Loss:0.0003907735552424185 Trained in 1.3344190120697021 seconds\n",
      "Epoch: 35 Loss:0.00021545410120715758 Trained in 1.339416265487671 seconds\n",
      "Epoch: 36 Loss:0.00038882548813745643 Trained in 1.3251800537109375 seconds\n",
      "Epoch: 37 Loss:0.0003032838410188532 Trained in 1.3493545055389404 seconds\n",
      "Epoch: 38 Loss:0.00025763255110433647 Trained in 1.349428653717041 seconds\n",
      "Epoch: 39 Loss:0.0004930342079418892 Trained in 1.3330469131469727 seconds\n",
      "Epoch: 40 Loss:0.00010414600452790523 Trained in 1.3407564163208008 seconds\n",
      "Epoch: 41 Loss:0.0002890300798430445 Trained in 1.348414421081543 seconds\n",
      "Epoch: 42 Loss:0.00022766883578206887 Trained in 1.3563759326934814 seconds\n",
      "Epoch: 43 Loss:0.00026355743194450554 Trained in 1.3275196552276611 seconds\n",
      "Epoch: 44 Loss:6.964683612586953e-05 Trained in 1.3334331512451172 seconds\n",
      "Epoch: 45 Loss:6.179332791944603e-05 Trained in 1.3505420684814453 seconds\n",
      "Epoch: 46 Loss:0.0007215099798418834 Trained in 1.3427205085754395 seconds\n",
      "Epoch: 47 Loss:0.0002455029210182147 Trained in 1.3726129531860352 seconds\n",
      "Epoch: 48 Loss:8.060455319913729e-05 Trained in 1.3282246589660645 seconds\n",
      "Epoch: 49 Loss:0.00016542911294514795 Trained in 1.3393888473510742 seconds\n",
      "Epoch: 50 Loss:0.00014998875896843344 Trained in 1.3513851165771484 seconds\n",
      "Epoch: 51 Loss:6.484985375365682e-05 Trained in 1.3512022495269775 seconds\n",
      "Epoch: 52 Loss:4.9767493550234576e-05 Trained in 1.3646845817565918 seconds\n",
      "Epoch: 53 Loss:3.115653948171371e-05 Trained in 1.3324358463287354 seconds\n",
      "Epoch: 54 Loss:8.697106303046098e-05 Trained in 1.3513846397399902 seconds\n",
      "Epoch: 55 Loss:0.000826816579850842 Trained in 1.3364250659942627 seconds\n",
      "Epoch: 56 Loss:2.6516914148899673e-05 Trained in 1.3493897914886475 seconds\n",
      "Epoch: 57 Loss:2.4294853282924578e-05 Trained in 1.347395896911621 seconds\n",
      "Epoch: 58 Loss:3.774679609946929e-05 Trained in 1.3583660125732422 seconds\n",
      "Epoch: 59 Loss:8.991608055630707e-05 Trained in 1.3483929634094238 seconds\n",
      "Epoch: 60 Loss:2.9458999771136973e-05 Trained in 1.3378183841705322 seconds\n",
      "Epoch: 61 Loss:3.1380653522816715e-05 Trained in 1.3653199672698975 seconds\n",
      "Epoch: 62 Loss:2.794742638201342e-05 Trained in 1.3389496803283691 seconds\n",
      "Epoch: 63 Loss:1.549720749771666e-05 Trained in 1.33941650390625 seconds\n",
      "Epoch: 64 Loss:0.00011142730759239328 Trained in 1.3600471019744873 seconds\n",
      "Epoch: 65 Loss:5.587577752486084e-05 Trained in 1.3244569301605225 seconds\n",
      "Epoch: 66 Loss:2.8152832330263777e-05 Trained in 1.3434059619903564 seconds\n",
      "Epoch: 67 Loss:0.00012243747359619306 Trained in 1.3354275226593018 seconds\n",
      "Epoch: 68 Loss:1.4983690462599952e-05 Trained in 1.320354700088501 seconds\n",
      "Epoch: 69 Loss:2.5939207505487616e-05 Trained in 1.3249378204345703 seconds\n",
      "Epoch: 70 Loss:5.527936408356027e-05 Trained in 1.3430848121643066 seconds\n",
      "Epoch: 71 Loss:7.305145224734133e-06 Trained in 1.3377656936645508 seconds\n",
      "Epoch: 72 Loss:1.6495631150803547e-05 Trained in 1.346100091934204 seconds\n",
      "Epoch: 73 Loss:3.059387205439634e-05 Trained in 1.326854944229126 seconds\n",
      "Epoch: 74 Loss:2.790927916329622e-05 Trained in 1.3426623344421387 seconds\n",
      "Epoch: 75 Loss:0.0006374225037344416 Trained in 1.352004051208496 seconds\n",
      "Epoch: 76 Loss:2.0623207115377795e-05 Trained in 1.3354270458221436 seconds\n",
      "Epoch: 77 Loss:1.9149046691424587e-05 Trained in 1.3524341583251953 seconds\n",
      "Epoch: 78 Loss:5.3191185081402637e-05 Trained in 1.3424575328826904 seconds\n",
      "Epoch: 79 Loss:1.6679764076599213e-05 Trained in 1.3481485843658447 seconds\n",
      "Epoch: 80 Loss:3.260685918604622e-05 Trained in 1.3130767345428467 seconds\n",
      "Epoch: 81 Loss:8.640656165681548e-06 Trained in 1.3309392929077148 seconds\n",
      "Epoch: 82 Loss:6.903134783797782e-06 Trained in 1.3510723114013672 seconds\n",
      "Epoch: 83 Loss:6.913662026697409e-05 Trained in 1.3521876335144043 seconds\n",
      "Epoch: 84 Loss:3.121852848941842e-05 Trained in 1.3613789081573486 seconds\n",
      "Epoch: 85 Loss:0.00010771751540517016 Trained in 1.389284372329712 seconds\n",
      "Epoch: 86 Loss:5.851525584077422e-06 Trained in 1.3628435134887695 seconds\n",
      "Epoch: 87 Loss:1.9884109763168567e-05 Trained in 1.3354265689849854 seconds\n",
      "Epoch: 88 Loss:5.354404524382517e-05 Trained in 1.3593635559082031 seconds\n",
      "Epoch: 89 Loss:6.110191486818906e-05 Trained in 1.3185293674468994 seconds\n",
      "Epoch: 90 Loss:5.191766255485675e-05 Trained in 1.364142656326294 seconds\n",
      "Epoch: 91 Loss:3.0005528255827585e-05 Trained in 1.359302043914795 seconds\n",
      "Epoch: 92 Loss:3.600120580316002e-06 Trained in 1.3486924171447754 seconds\n",
      "Epoch: 93 Loss:5.302429153530852e-06 Trained in 1.3580396175384521 seconds\n",
      "Epoch: 94 Loss:2.2344589815759264e-05 Trained in 1.3264448642730713 seconds\n",
      "Epoch: 95 Loss:5.5951339028581515e-06 Trained in 1.3683342933654785 seconds\n",
      "Epoch: 96 Loss:2.3078918260921455e-06 Trained in 1.3434276580810547 seconds\n",
      "Epoch: 97 Loss:5.750655990510722e-06 Trained in 1.3520004749298096 seconds\n",
      "Epoch: 98 Loss:1.0910034138333913e-05 Trained in 1.34039306640625 seconds\n",
      "Epoch: 99 Loss:5.733489874160114e-05 Trained in 1.3621635437011719 seconds\n",
      "Epoch: 100 Loss:2.1171569741795793e-06 Trained in 1.3341717720031738 seconds\n",
      "================================  Partition:3  ======================================\n",
      "Epoch: 1 Loss:3.4845087567064184 Trained in 1.3214986324310303 seconds\n",
      "Epoch: 2 Loss:0.0988821436053513 Trained in 1.3314802646636963 seconds\n",
      "Epoch: 3 Loss:0.02948330322006143 Trained in 1.3278541564941406 seconds\n",
      "Epoch: 4 Loss:0.005769300405830791 Trained in 1.3314416408538818 seconds\n",
      "Epoch: 5 Loss:0.003104846482138157 Trained in 1.3415355682373047 seconds\n",
      "Epoch: 6 Loss:0.005412575631028815 Trained in 1.329657793045044 seconds\n",
      "Epoch: 7 Loss:0.001304603539502125 Trained in 1.3399202823638916 seconds\n",
      "Epoch: 8 Loss:0.006235853202237962 Trained in 1.336395263671875 seconds\n",
      "Epoch: 9 Loss:0.2069806465836912 Trained in 1.3513855934143066 seconds\n",
      "Epoch: 10 Loss:0.025467103774701627 Trained in 1.3417844772338867 seconds\n",
      "Epoch: 11 Loss:0.016623464246094954 Trained in 1.3394381999969482 seconds\n",
      "Epoch: 12 Loss:0.008106690695422358 Trained in 1.3290281295776367 seconds\n",
      "Epoch: 13 Loss:0.09545602953314614 Trained in 1.3429877758026123 seconds\n",
      "Epoch: 14 Loss:0.20608495551152828 Trained in 1.3334331512451172 seconds\n",
      "Epoch: 15 Loss:0.02748792919447851 Trained in 1.3396575450897217 seconds\n",
      "Epoch: 16 Loss:0.0027054502447061424 Trained in 1.3054351806640625 seconds\n",
      "Epoch: 17 Loss:0.004063330940713428 Trained in 1.3365256786346436 seconds\n",
      "Epoch: 18 Loss:0.0012064456684761637 Trained in 1.3339226245880127 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Loss:0.0037093211834449136 Trained in 1.3643217086791992 seconds\n",
      "Epoch: 20 Loss:0.0007990308871530516 Trained in 1.341423749923706 seconds\n",
      "Epoch: 21 Loss:0.0001877927773712429 Trained in 1.3423168659210205 seconds\n",
      "Epoch: 22 Loss:0.003916958555455707 Trained in 1.3493905067443848 seconds\n",
      "Epoch: 23 Loss:0.0019196721587455556 Trained in 1.352710485458374 seconds\n",
      "Epoch: 24 Loss:0.00013282299240913176 Trained in 1.3282749652862549 seconds\n",
      "Epoch: 25 Loss:0.0012621283381282211 Trained in 1.3369956016540527 seconds\n",
      "Epoch: 26 Loss:0.00015473585905745324 Trained in 1.342428207397461 seconds\n",
      "Epoch: 27 Loss:0.0006210326979925185 Trained in 1.3697104454040527 seconds\n",
      "Epoch: 28 Loss:8.072376194689923e-05 Trained in 1.3543980121612549 seconds\n",
      "Epoch: 29 Loss:0.0003444136121473207 Trained in 1.338829755783081 seconds\n",
      "Epoch: 30 Loss:3.2444000330755784e-05 Trained in 1.338538408279419 seconds\n",
      "Epoch: 31 Loss:0.000250706668571965 Trained in 1.3476207256317139 seconds\n",
      "Epoch: 32 Loss:0.00012844489267926917 Trained in 1.331629753112793 seconds\n",
      "Epoch: 33 Loss:0.0003046563987965101 Trained in 1.3482022285461426 seconds\n",
      "Epoch: 34 Loss:0.00030886173045630017 Trained in 1.3384404182434082 seconds\n",
      "Epoch: 35 Loss:0.00015603542686903893 Trained in 1.338409662246704 seconds\n",
      "Epoch: 36 Loss:0.0006581878569882349 Trained in 1.3450744152069092 seconds\n",
      "Epoch: 37 Loss:6.866455198206722e-05 Trained in 1.3434009552001953 seconds\n",
      "Epoch: 38 Loss:0.00011196136561686387 Trained in 1.336458444595337 seconds\n",
      "Epoch: 39 Loss:3.412246671885555e-05 Trained in 1.3424568176269531 seconds\n",
      "Epoch: 40 Loss:0.00014246940299855737 Trained in 1.3718018531799316 seconds\n",
      "Epoch: 41 Loss:1.655101793751612e-05 Trained in 1.3354272842407227 seconds\n",
      "Epoch: 42 Loss:5.857467670900007e-05 Trained in 1.3593792915344238 seconds\n",
      "Epoch: 43 Loss:6.95514656641194e-05 Trained in 1.3307111263275146 seconds\n",
      "Epoch: 44 Loss:5.175590524331142e-05 Trained in 1.359057903289795 seconds\n",
      "Epoch: 45 Loss:0.0003628848282470898 Trained in 1.3454008102416992 seconds\n",
      "Epoch: 46 Loss:0.00036901474675410384 Trained in 1.3374555110931396 seconds\n",
      "Epoch: 47 Loss:2.3607107110024117e-05 Trained in 1.344219446182251 seconds\n",
      "Epoch: 48 Loss:0.00018043115080423888 Trained in 1.364349126815796 seconds\n",
      "Epoch: 49 Loss:0.00012463092528669506 Trained in 1.3374223709106445 seconds\n",
      "Epoch: 50 Loss:1.9636153931301692e-05 Trained in 1.3533687591552734 seconds\n",
      "Epoch: 51 Loss:3.3630224150371646e-05 Trained in 1.354867696762085 seconds\n",
      "Epoch: 52 Loss:7.129669265992788e-05 Trained in 1.3522577285766602 seconds\n",
      "Epoch: 53 Loss:2.611527081164411e-05 Trained in 1.3344295024871826 seconds\n",
      "Epoch: 54 Loss:7.972717339654878e-06 Trained in 1.332038402557373 seconds\n",
      "Epoch: 55 Loss:2.780620831188685e-05 Trained in 1.3523824214935303 seconds\n",
      "Epoch: 56 Loss:9.463786710739441e-05 Trained in 1.322026014328003 seconds\n",
      "Epoch: 57 Loss:0.0001511573781147746 Trained in 1.3590807914733887 seconds\n",
      "Epoch: 58 Loss:3.451824250255697e-05 Trained in 1.3481245040893555 seconds\n",
      "Epoch: 59 Loss:0.0001053802806918469 Trained in 1.3257865905761719 seconds\n",
      "Epoch: 60 Loss:3.386020581963578e-05 Trained in 1.3665282726287842 seconds\n",
      "Epoch: 61 Loss:0.0005605983817638105 Trained in 1.346398115158081 seconds\n",
      "Epoch: 62 Loss:4.767528079874239e-05 Trained in 1.3513846397399902 seconds\n",
      "Epoch: 63 Loss:3.001690012460756e-05 Trained in 1.3509666919708252 seconds\n",
      "Epoch: 64 Loss:1.7714500250676224e-05 Trained in 1.3396649360656738 seconds\n",
      "Epoch: 65 Loss:6.305987993115991e-06 Trained in 1.3446519374847412 seconds\n",
      "Epoch: 66 Loss:1.181602462452247e-05 Trained in 1.390279769897461 seconds\n",
      "Epoch: 67 Loss:4.692077565593422e-06 Trained in 1.3392932415008545 seconds\n",
      "Epoch: 68 Loss:5.5360794434022864e-05 Trained in 1.359363317489624 seconds\n",
      "Epoch: 69 Loss:9.212494118671088e-06 Trained in 1.328446388244629 seconds\n",
      "Epoch: 70 Loss:7.139205840189788e-05 Trained in 1.3675963878631592 seconds\n",
      "Epoch: 71 Loss:9.076045113687314e-06 Trained in 1.3503851890563965 seconds\n",
      "Epoch: 72 Loss:7.237764473622121e-05 Trained in 1.3391587734222412 seconds\n",
      "Epoch: 73 Loss:2.689361540575419e-06 Trained in 1.3443927764892578 seconds\n",
      "Epoch: 74 Loss:7.840633624311977e-05 Trained in 1.33137845993042 seconds\n",
      "Epoch: 75 Loss:3.3421516015152974e-05 Trained in 1.3579728603363037 seconds\n",
      "Epoch: 76 Loss:8.153915436182047e-06 Trained in 1.3204669952392578 seconds\n",
      "Epoch: 77 Loss:0.0001264612520603947 Trained in 1.3533964157104492 seconds\n",
      "Epoch: 78 Loss:1.4925002746579707e-05 Trained in 1.3292486667633057 seconds\n",
      "Epoch: 79 Loss:1.1692047086953039e-05 Trained in 1.3613591194152832 seconds\n",
      "Epoch: 80 Loss:4.786968283809756e-05 Trained in 1.347700595855713 seconds\n",
      "Epoch: 81 Loss:5.4981524453268094e-05 Trained in 1.3479852676391602 seconds\n",
      "Epoch: 82 Loss:3.2711029049892204e-06 Trained in 1.328446626663208 seconds\n",
      "Epoch: 83 Loss:3.4332275742343654e-06 Trained in 1.3661513328552246 seconds\n",
      "Epoch: 84 Loss:1.2569427049413662e-05 Trained in 1.3335764408111572 seconds\n",
      "Epoch: 85 Loss:0.0012580561731336815 Trained in 1.326763391494751 seconds\n",
      "Epoch: 86 Loss:6.4468384302074355e-06 Trained in 1.343458890914917 seconds\n",
      "Epoch: 87 Loss:1.7018317841177577e-05 Trained in 1.3196337223052979 seconds\n",
      "Epoch: 88 Loss:2.5806427869312643e-05 Trained in 1.3537895679473877 seconds\n",
      "Epoch: 89 Loss:0.00018640994966290236 Trained in 1.3749916553497314 seconds\n",
      "Epoch: 90 Loss:1.0776519912880644e-05 Trained in 1.3530535697937012 seconds\n",
      "Epoch: 91 Loss:4.005432110432139e-07 Trained in 1.3243420124053955 seconds\n",
      "Epoch: 92 Loss:5.380923861153519e-06 Trained in 1.3373935222625732 seconds\n",
      "Epoch: 93 Loss:2.136230435212383e-06 Trained in 1.31437349319458 seconds\n",
      "Epoch: 94 Loss:3.8623809466287184e-06 Trained in 1.3543767929077148 seconds\n",
      "Epoch: 95 Loss:1.7547607189527525e-06 Trained in 1.3279576301574707 seconds\n",
      "Epoch: 96 Loss:3.509521551592343e-06 Trained in 1.3354227542877197 seconds\n",
      "Epoch: 97 Loss:2.5558471161701846e-06 Trained in 1.3683600425720215 seconds\n",
      "Epoch: 98 Loss:2.231597859392309e-06 Trained in 1.3454127311706543 seconds\n",
      "Epoch: 99 Loss:4.644394007158326e-06 Trained in 1.360626220703125 seconds\n",
      "Epoch: 100 Loss:1.8978118667689614e-06 Trained in 1.3623430728912354 seconds\n",
      "================================  Partition:4  ======================================\n",
      "Epoch: 1 Loss:2.8499334764472035 Trained in 1.3723492622375488 seconds\n",
      "Epoch: 2 Loss:0.15765817201281607 Trained in 1.3683385848999023 seconds\n",
      "Epoch: 3 Loss:0.018574651940184594 Trained in 1.321378469467163 seconds\n",
      "Epoch: 4 Loss:0.004802063424676817 Trained in 1.3520176410675049 seconds\n",
      "Epoch: 5 Loss:0.0007221984890986022 Trained in 1.3164775371551514 seconds\n",
      "Epoch: 6 Loss:0.0006090685052679845 Trained in 1.3683390617370605 seconds\n",
      "Epoch: 7 Loss:0.0003748028058208064 Trained in 1.358365535736084 seconds\n",
      "Epoch: 8 Loss:0.0007267189107142968 Trained in 1.3493897914886475 seconds\n",
      "Epoch: 9 Loss:0.00034986385920987573 Trained in 1.3739428520202637 seconds\n",
      "Epoch: 10 Loss:5.563259085050731e-05 Trained in 1.369408130645752 seconds\n",
      "Epoch: 11 Loss:0.001672690593872872 Trained in 1.3936927318572998 seconds\n",
      "Epoch: 12 Loss:0.00029452801320850597 Trained in 1.3344299793243408 seconds\n",
      "Epoch: 13 Loss:0.00395184746265187 Trained in 1.3932640552520752 seconds\n",
      "Epoch: 14 Loss:0.04319286749323403 Trained in 1.330683708190918 seconds\n",
      "Epoch: 15 Loss:0.6472873153382839 Trained in 1.3621397018432617 seconds\n",
      "Epoch: 16 Loss:0.12268852747547498 Trained in 1.4331660270690918 seconds\n",
      "Epoch: 17 Loss:0.02043305497829806 Trained in 1.4311723709106445 seconds\n",
      "Epoch: 18 Loss:0.0019601664599626645 Trained in 1.3753211498260498 seconds\n",
      "Epoch: 19 Loss:0.001696116567545758 Trained in 1.385284662246704 seconds\n",
      "Epoch: 20 Loss:0.000455831380348215 Trained in 1.3593628406524658 seconds\n",
      "Epoch: 21 Loss:0.0013411645265328787 Trained in 1.4351613521575928 seconds\n",
      "Epoch: 22 Loss:0.0003443941772864889 Trained in 1.3952672481536865 seconds\n",
      "Epoch: 23 Loss:0.0005000400513353043 Trained in 1.3463985919952393 seconds\n",
      "Epoch: 24 Loss:0.00045366067817909084 Trained in 1.3593642711639404 seconds\n",
      "Epoch: 25 Loss:0.0009115695876360519 Trained in 1.3473951816558838 seconds\n",
      "Epoch: 26 Loss:0.0001847204752589704 Trained in 1.3543777465820312 seconds\n",
      "Epoch: 27 Loss:0.00334210163650539 Trained in 1.3333697319030762 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 Loss:0.002015552520873598 Trained in 1.323469638824463 seconds\n",
      "Epoch: 29 Loss:0.00013881683010552592 Trained in 1.387373924255371 seconds\n",
      "Epoch: 30 Loss:0.00016221046035802544 Trained in 1.3334298133850098 seconds\n",
      "Epoch: 31 Loss:0.0008724689447952017 Trained in 1.3344640731811523 seconds\n",
      "Epoch: 32 Loss:5.7258607030874487e-05 Trained in 1.322188377380371 seconds\n",
      "Epoch: 33 Loss:0.00026570027241668015 Trained in 1.3531088829040527 seconds\n",
      "Epoch: 34 Loss:1.184536869920482 Trained in 1.3350272178649902 seconds\n",
      "Epoch: 35 Loss:0.24735834460443584 Trained in 1.3553738594055176 seconds\n",
      "Epoch: 36 Loss:0.11125826980651254 Trained in 1.3255865573883057 seconds\n",
      "Epoch: 37 Loss:0.07342778420968443 Trained in 1.3367033004760742 seconds\n",
      "Epoch: 38 Loss:0.01600542012403139 Trained in 1.3291068077087402 seconds\n",
      "Epoch: 39 Loss:0.0033457321118817163 Trained in 1.3460056781768799 seconds\n",
      "Epoch: 40 Loss:0.003113849493317389 Trained in 1.3250155448913574 seconds\n",
      "Epoch: 41 Loss:0.03136140243017138 Trained in 1.348393201828003 seconds\n",
      "Epoch: 42 Loss:0.009493994005431716 Trained in 1.339442491531372 seconds\n",
      "Epoch: 43 Loss:0.007352214142185787 Trained in 1.3379857540130615 seconds\n",
      "Epoch: 44 Loss:0.0008709566347082642 Trained in 1.3558363914489746 seconds\n",
      "Epoch: 45 Loss:0.0023626717619062987 Trained in 1.3493895530700684 seconds\n",
      "Epoch: 46 Loss:0.0004184946666168088 Trained in 1.3475310802459717 seconds\n",
      "Epoch: 47 Loss:0.00028414616977379126 Trained in 1.318256139755249 seconds\n",
      "Epoch: 48 Loss:0.0009866714507094088 Trained in 1.3492870330810547 seconds\n",
      "Epoch: 49 Loss:0.00019370078934066726 Trained in 1.329967975616455 seconds\n",
      "Epoch: 50 Loss:0.0004766058957450525 Trained in 1.3298125267028809 seconds\n",
      "Epoch: 51 Loss:0.00014912605282901836 Trained in 1.331437587738037 seconds\n",
      "Epoch: 52 Loss:0.00010411629106776843 Trained in 1.3335912227630615 seconds\n",
      "Epoch: 53 Loss:0.0002134898984991196 Trained in 1.3324356079101562 seconds\n",
      "Epoch: 54 Loss:0.00018452644229860482 Trained in 1.320744514465332 seconds\n",
      "Epoch: 55 Loss:0.00026360034933858856 Trained in 1.3623785972595215 seconds\n",
      "Epoch: 56 Loss:0.00020550728112667116 Trained in 1.3364362716674805 seconds\n",
      "Epoch: 57 Loss:5.1114009449904074e-05 Trained in 1.3394157886505127 seconds\n",
      "Epoch: 58 Loss:9.925365426077803e-05 Trained in 1.3334319591522217 seconds\n",
      "Epoch: 59 Loss:0.00014866352202425048 Trained in 1.3294970989227295 seconds\n",
      "Epoch: 60 Loss:3.8299560172205815e-05 Trained in 1.3409976959228516 seconds\n",
      "Epoch: 61 Loss:0.00015379429266104694 Trained in 1.3231899738311768 seconds\n",
      "Epoch: 62 Loss:6.328105862252187e-05 Trained in 1.3454005718231201 seconds\n",
      "Epoch: 63 Loss:4.536958825518411e-05 Trained in 1.3518712520599365 seconds\n",
      "Epoch: 64 Loss:0.0004104915158098521 Trained in 1.3365483283996582 seconds\n",
      "Epoch: 65 Loss:5.001068112520102e-05 Trained in 1.3312053680419922 seconds\n",
      "Epoch: 66 Loss:3.8716242565683956e-05 Trained in 1.6336297988891602 seconds\n",
      "Epoch: 67 Loss:2.2133681056857313e-05 Trained in 1.3701329231262207 seconds\n",
      "Epoch: 68 Loss:0.00021819591129101923 Trained in 1.3708670139312744 seconds\n",
      "Epoch: 69 Loss:2.981662835743748e-05 Trained in 1.3494277000427246 seconds\n",
      "Epoch: 70 Loss:3.0331611785783252e-05 Trained in 1.3553950786590576 seconds\n",
      "Epoch: 71 Loss:3.390789061086252e-05 Trained in 1.3250231742858887 seconds\n",
      "Epoch: 72 Loss:0.00013006210319410627 Trained in 1.367342472076416 seconds\n",
      "Epoch: 73 Loss:0.00015711637414561608 Trained in 1.3643670082092285 seconds\n",
      "Epoch: 74 Loss:0.000556402229880959 Trained in 1.3300402164459229 seconds\n",
      "Epoch: 75 Loss:1.859664916636916e-05 Trained in 1.3414130210876465 seconds\n",
      "Epoch: 76 Loss:5.61824213303197e-05 Trained in 1.3428349494934082 seconds\n",
      "Epoch: 77 Loss:0.0041724586496254545 Trained in 1.3413970470428467 seconds\n",
      "Epoch: 78 Loss:0.19685637938133915 Trained in 1.3403937816619873 seconds\n",
      "Epoch: 79 Loss:0.3476773729909155 Trained in 1.3378245830535889 seconds\n",
      "Epoch: 80 Loss:0.013792652994343513 Trained in 1.333465337753296 seconds\n",
      "Epoch: 81 Loss:0.005102096803657474 Trained in 1.3328726291656494 seconds\n",
      "Epoch: 82 Loss:0.0010868453919083976 Trained in 1.3404133319854736 seconds\n",
      "Epoch: 83 Loss:0.0022256824052035995 Trained in 1.3219404220581055 seconds\n",
      "Epoch: 84 Loss:0.00047893046941815953 Trained in 1.3174755573272705 seconds\n",
      "Epoch: 85 Loss:0.0011079868795285108 Trained in 1.320441484451294 seconds\n",
      "Epoch: 86 Loss:0.005137314747592825 Trained in 1.3503689765930176 seconds\n",
      "Epoch: 87 Loss:0.21241086536453757 Trained in 1.3484165668487549 seconds\n",
      "Epoch: 88 Loss:0.008141353906232496 Trained in 1.3625967502593994 seconds\n",
      "Epoch: 89 Loss:0.0010972699936306896 Trained in 1.331014633178711 seconds\n",
      "Epoch: 90 Loss:0.0008670091679974234 Trained in 1.3334324359893799 seconds\n",
      "Epoch: 91 Loss:0.0006289896483409052 Trained in 1.3453996181488037 seconds\n",
      "Epoch: 92 Loss:0.024020566622914075 Trained in 1.3412501811981201 seconds\n",
      "Epoch: 93 Loss:0.008076486921876125 Trained in 1.3444018363952637 seconds\n",
      "Epoch: 94 Loss:0.0002171501815269039 Trained in 1.352994441986084 seconds\n",
      "Epoch: 95 Loss:1.4019012304800071e-05 Trained in 1.3524041175842285 seconds\n",
      "Epoch: 96 Loss:0.28255796798185884 Trained in 1.3776187896728516 seconds\n",
      "Epoch: 97 Loss:0.40341759316503456 Trained in 1.3474187850952148 seconds\n",
      "Epoch: 98 Loss:0.2206386559119924 Trained in 1.3474211692810059 seconds\n",
      "Epoch: 99 Loss:0.034438100826339024 Trained in 1.3933453559875488 seconds\n",
      "Epoch: 100 Loss:0.019698849077682823 Trained in 1.3378629684448242 seconds\n",
      "================================  Partition:5  ======================================\n",
      "Epoch: 1 Loss:1.2214750647282244 Trained in 1.3447554111480713 seconds\n",
      "Epoch: 2 Loss:0.10792971301702892 Trained in 1.32551908493042 seconds\n",
      "Epoch: 3 Loss:0.23819618367339856 Trained in 1.3262622356414795 seconds\n",
      "Epoch: 4 Loss:0.18681575897562652 Trained in 1.3436667919158936 seconds\n",
      "Epoch: 5 Loss:0.09823711951970893 Trained in 1.3531572818756104 seconds\n",
      "Epoch: 6 Loss:0.03759885937977003 Trained in 1.3543775081634521 seconds\n",
      "Epoch: 7 Loss:0.0035340228253399886 Trained in 1.3224718570709229 seconds\n",
      "Epoch: 8 Loss:0.0005242688423443553 Trained in 1.3743441104888916 seconds\n",
      "Epoch: 9 Loss:0.002080512008840074 Trained in 1.3315536975860596 seconds\n",
      "Epoch: 10 Loss:0.008315106831826569 Trained in 1.3284454345703125 seconds\n",
      "Epoch: 11 Loss:0.22809259601541143 Trained in 1.3350739479064941 seconds\n",
      "Epoch: 12 Loss:0.14564846977613044 Trained in 1.3284456729888916 seconds\n",
      "Epoch: 13 Loss:0.27407217148808627 Trained in 1.3532934188842773 seconds\n",
      "Epoch: 14 Loss:0.039131840914269134 Trained in 1.325453758239746 seconds\n",
      "Epoch: 15 Loss:0.0056178382529328275 Trained in 1.331449031829834 seconds\n",
      "Epoch: 16 Loss:0.00823975812472355 Trained in 1.3194692134857178 seconds\n",
      "Epoch: 17 Loss:0.0036845875220219426 Trained in 1.3633527755737305 seconds\n",
      "Epoch: 18 Loss:0.00192112920288956 Trained in 1.3430802822113037 seconds\n",
      "Epoch: 19 Loss:0.00023479665861536603 Trained in 1.3493895530700684 seconds\n",
      "Epoch: 20 Loss:0.0009670686657727856 Trained in 1.3493900299072266 seconds\n",
      "Epoch: 21 Loss:0.0006935596487949169 Trained in 1.3254539966583252 seconds\n",
      "Epoch: 22 Loss:0.0004898977346137201 Trained in 1.3443965911865234 seconds\n",
      "Epoch: 23 Loss:0.0001237678544505627 Trained in 1.3433928489685059 seconds\n",
      "Epoch: 24 Loss:0.0008597516706956299 Trained in 1.337327003479004 seconds\n",
      "Epoch: 25 Loss:0.0006204455200169434 Trained in 1.348393440246582 seconds\n",
      "Epoch: 26 Loss:0.0006628778975308336 Trained in 1.3533663749694824 seconds\n",
      "Epoch: 27 Loss:0.0001165914564911219 Trained in 1.3278343677520752 seconds\n",
      "Epoch: 28 Loss:0.00025130136337914166 Trained in 1.3713715076446533 seconds\n",
      "Epoch: 29 Loss:0.00011129447603508424 Trained in 1.3320393562316895 seconds\n",
      "Epoch: 30 Loss:0.0007062023069508427 Trained in 1.3653473854064941 seconds\n",
      "Epoch: 31 Loss:0.00011504650086280321 Trained in 1.3523826599121094 seconds\n",
      "Epoch: 32 Loss:0.00017309665850895328 Trained in 1.3384194374084473 seconds\n",
      "Epoch: 33 Loss:0.14212279736390698 Trained in 1.3424017429351807 seconds\n",
      "Epoch: 34 Loss:0.026772551883635032 Trained in 1.3294053077697754 seconds\n",
      "Epoch: 35 Loss:0.0028070163265674353 Trained in 1.3611540794372559 seconds\n",
      "Epoch: 36 Loss:0.002441914804906631 Trained in 1.3323640823364258 seconds\n",
      "Epoch: 37 Loss:0.001031877988154406 Trained in 1.3384194374084473 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 Loss:0.0031320929386087215 Trained in 1.344407081604004 seconds\n",
      "Epoch: 39 Loss:0.0003174448048532952 Trained in 1.3673427104949951 seconds\n",
      "Epoch: 40 Loss:0.02398973986530173 Trained in 1.3414113521575928 seconds\n",
      "Epoch: 41 Loss:0.0003732252102359723 Trained in 1.3463845252990723 seconds\n",
      "Epoch: 42 Loss:0.00031782150566783685 Trained in 1.3414289951324463 seconds\n",
      "Epoch: 43 Loss:0.0004067230122135612 Trained in 1.332566261291504 seconds\n",
      "Epoch: 44 Loss:0.0006164169289082366 Trained in 1.3633527755737305 seconds\n",
      "Epoch: 45 Loss:0.001144552221152395 Trained in 1.3383445739746094 seconds\n",
      "Epoch: 46 Loss:0.00038678170977313187 Trained in 1.3292758464813232 seconds\n",
      "Epoch: 47 Loss:0.05567924975606431 Trained in 1.3449993133544922 seconds\n",
      "Epoch: 48 Loss:0.34999020473690834 Trained in 1.3454177379608154 seconds\n",
      "Epoch: 49 Loss:1.4491265268658395 Trained in 1.3374226093292236 seconds\n",
      "Epoch: 50 Loss:0.5390910961359623 Trained in 1.3274757862091064 seconds\n",
      "Epoch: 51 Loss:0.11590845302713149 Trained in 1.332434892654419 seconds\n",
      "Epoch: 52 Loss:0.029898056278479856 Trained in 1.3304312229156494 seconds\n",
      "Epoch: 53 Loss:0.002134819695658763 Trained in 1.3407261371612549 seconds\n",
      "Epoch: 54 Loss:0.0028461121812952683 Trained in 1.3224685192108154 seconds\n",
      "Epoch: 55 Loss:8.478505311693141e-05 Trained in 1.3626704216003418 seconds\n",
      "Epoch: 56 Loss:0.0006181383255121631 Trained in 1.3663442134857178 seconds\n",
      "Epoch: 57 Loss:0.004330631097504423 Trained in 1.3920328617095947 seconds\n",
      "Epoch: 58 Loss:5.9440475949301685e-05 Trained in 1.3763463497161865 seconds\n",
      "Epoch: 59 Loss:0.0013251352029595154 Trained in 1.3261377811431885 seconds\n",
      "Epoch: 60 Loss:0.00017587321007184187 Trained in 1.3424086570739746 seconds\n",
      "Epoch: 61 Loss:5.469322116802289e-05 Trained in 1.3414604663848877 seconds\n",
      "Epoch: 62 Loss:0.002080647973329519 Trained in 1.3491628170013428 seconds\n",
      "Epoch: 63 Loss:0.00011075905233504102 Trained in 1.6276297569274902 seconds\n",
      "Epoch: 64 Loss:7.385730666342738e-05 Trained in 1.500983715057373 seconds\n",
      "Epoch: 65 Loss:3.590515651907822e-05 Trained in 1.3713319301605225 seconds\n",
      "Epoch: 66 Loss:2.0217895739804703e-05 Trained in 1.3432159423828125 seconds\n",
      "Epoch: 67 Loss:1.5630721993176167e-05 Trained in 1.3992571830749512 seconds\n",
      "Epoch: 68 Loss:0.00489900966426049 Trained in 1.4069116115570068 seconds\n",
      "Epoch: 69 Loss:1.9241060268626597e-05 Trained in 1.3978636264801025 seconds\n",
      "Epoch: 70 Loss:4.4469833120786006e-05 Trained in 1.3603606224060059 seconds\n",
      "Epoch: 71 Loss:1.657485964834393e-05 Trained in 1.3542344570159912 seconds\n",
      "Epoch: 72 Loss:8.306503277211164e-06 Trained in 1.3733265399932861 seconds\n",
      "Epoch: 73 Loss:2.4394989146969692e-05 Trained in 1.329444408416748 seconds\n",
      "Epoch: 74 Loss:5.1403045521425383e-05 Trained in 1.334089994430542 seconds\n",
      "Epoch: 75 Loss:0.00022132396750151884 Trained in 1.3803720474243164 seconds\n",
      "Epoch: 76 Loss:4.44173825844274e-05 Trained in 1.3560452461242676 seconds\n",
      "Epoch: 77 Loss:1.3224737957173716e-05 Trained in 1.3567814826965332 seconds\n",
      "Epoch: 78 Loss:4.149709379497324e-05 Trained in 1.331005334854126 seconds\n",
      "Epoch: 79 Loss:5.0921441197004924e-05 Trained in 1.3541860580444336 seconds\n",
      "Epoch: 80 Loss:5.036354090393047e-05 Trained in 1.3729100227355957 seconds\n",
      "Epoch: 81 Loss:8.417606275301637e-05 Trained in 1.3726537227630615 seconds\n",
      "Epoch: 82 Loss:0.00011443615036377253 Trained in 1.3573544025421143 seconds\n",
      "Epoch: 83 Loss:0.0002231883954770808 Trained in 1.3872857093811035 seconds\n",
      "Epoch: 84 Loss:2.521038108227458e-05 Trained in 1.4087960720062256 seconds\n",
      "Epoch: 85 Loss:4.544734834688313e-05 Trained in 1.3554155826568604 seconds\n",
      "Epoch: 86 Loss:3.428459183574262e-05 Trained in 1.3374395370483398 seconds\n",
      "Epoch: 87 Loss:2.5048255654525065e-05 Trained in 1.3763184547424316 seconds\n",
      "Epoch: 88 Loss:2.2525787084504145e-05 Trained in 1.3593635559082031 seconds\n",
      "Epoch: 89 Loss:2.7670861124207136e-05 Trained in 1.3573689460754395 seconds\n",
      "Epoch: 90 Loss:0.00016699791114049845 Trained in 1.3683345317840576 seconds\n",
      "Epoch: 91 Loss:4.405975394305983e-06 Trained in 1.3803141117095947 seconds\n",
      "Epoch: 92 Loss:2.0599364960460775e-05 Trained in 1.3562395572662354 seconds\n",
      "Epoch: 93 Loss:4.091262814753804e-06 Trained in 1.3563706874847412 seconds\n",
      "Epoch: 94 Loss:4.358291723960406e-06 Trained in 1.357125997543335 seconds\n",
      "Epoch: 95 Loss:7.6198579304076475e-06 Trained in 1.3524575233459473 seconds\n",
      "Epoch: 96 Loss:8.525848382845425e-06 Trained in 1.5578324794769287 seconds\n",
      "Epoch: 97 Loss:8.18252571299638e-06 Trained in 1.4164071083068848 seconds\n",
      "Epoch: 98 Loss:4.891872448098411e-05 Trained in 1.3503587245941162 seconds\n",
      "Epoch: 99 Loss:1.4276504456844918e-05 Trained in 1.4291257858276367 seconds\n",
      "Epoch: 100 Loss:0.0007798249198263107 Trained in 1.366330862045288 seconds\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "losses_per_dataset = []\n",
    "partition = 0\n",
    "for dataset in partitions:\n",
    "    partition += 1\n",
    "    print(f\"================================  Partition:{partition}  ======================================\")\n",
    "    train_dataset = utils.TensorDataset(image_tensor[dataset[0]:dataset[1]], label_tensor[dataset[0]:dataset[1]])\n",
    "    train_dataloader = utils.DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "    losses_per_epoch = []\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        t0 = time.time()\n",
    "        losses_per_batch = []\n",
    "\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            patches, labels = data\n",
    "\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            output = model(patches.cuda().float())\n",
    "\n",
    "            # calculate batch loss\n",
    "            loss = criterion(output, labels.cuda().long())\n",
    "            # compute \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses_per_batch.append(loss.item())\n",
    "\n",
    "        t1 = time.time()\n",
    "        losses_per_epoch.append(sum(losses_per_batch))\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Epoch: {0} Loss:{1} Trained in {2} seconds\".format(epoch+1, sum(losses_per_batch), t1-t0))\n",
    "    \n",
    "    losses_per_dataset.append(losses_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train the data for **epochs** on mini-batches of **batch_size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"5class_normalised_1streak\" + \".sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name the model vessels_crossentropyloss_1streak10epochs\n",
      "Saved as vessels_crossentropyloss_1streak10epochs.sav\n"
     ]
    }
   ],
   "source": [
    "model_name = input(\"Name the model \")\n",
    "torch.save(model.state_dict(), model_name + \".sav\")\n",
    "print(f\"Saved as {model_name}.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recolour(image):\n",
    "    colours = np.array([\n",
    "        [0,0,0],\n",
    "        [0,255,0],\n",
    "        [0,0,255],\n",
    "        [255,0,0],\n",
    "        [255,0,255],\n",
    "    ])\n",
    "    \n",
    "    colour_vector = np.take(colours, image.flatten(), axis=0)\n",
    "    colour_vector = np.reshape(colour_vector, (image.shape[0], image.shape[1], 3))\n",
    "    \n",
    "    return colour_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_test, feature_test = load_numpy_data(\"Test\")\n",
    "image_test, feature_test = load_numpy_data_task1_specificfeature(\"Test\", \"masks_Soft_Exudates\", \"SE\")\n",
    "# image_test = np.array(image_test)\n",
    "\n",
    "# image_test_mean = np.mean(image_test, axis=tuple(range(image_test.ndim-1)))\n",
    "# image_test_std = np.std(image_test, axis=tuple(range(image_test.ndim-1)))\n",
    "\n",
    "# image_test = image_test - image_test_mean\n",
    "# image_test = image_test / image_test_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "\n",
    "\n",
    "def test(num, epochs, task, name):\n",
    "    model.eval()\n",
    "    image, truth = list(zip(image_test, feature_test))[num]\n",
    "    patches = imgutil.extract_patches_2d(image, (32,32))\n",
    "    rolled_patches = [torch.Tensor(np.rollaxis(patch,2,0)) for patch in patches]\n",
    "    rolled_patches_tensor = torch.stack(rolled_patches)\n",
    "    image_patches_dataset = utils.TensorDataset(rolled_patches_tensor)\n",
    "    image_loader = utils.DataLoader(image_patches_dataset, batch_size=225)\n",
    "\n",
    "    generated_mask = []\n",
    "\n",
    "    for i, image_patch_ in enumerate(image_loader):\n",
    "        img_patch = image_patch_[0]\n",
    "        test_output = model(img_patch.cuda())\n",
    "        labels = torch.argmax(test_output,1)# convert one hot to index/pixel form\n",
    "        generated_mask.append(labels.cpu().data.numpy())\n",
    "\n",
    "    generated_mask = np.array(generated_mask)\n",
    "    \n",
    "    coloured = recolour(generated_mask)\n",
    "    \n",
    "\n",
    "    \n",
    "    coloured_truth = recolour(truth[16:16+225,16:16+225])\n",
    "\n",
    "    side_by_side = np.hstack((coloured_truth, coloured, image[16:16+225,16:16+225]))\n",
    "    cv2.imwrite(f\"outputs/{task}/{name}/test{num}_{name}_{epochs}.jpg\", side_by_side)\n",
    "    \n",
    "    with open(f'outputs/{task}/{name}/test{num}_{name}_{epochs}.sav', 'wb') as file:\n",
    "        pkl.dump({\n",
    "            'pred': generated_mask,\n",
    "            'truth': truth,\n",
    "            'sideBySide': side_by_side,\n",
    "            'trainingLoss': losses_per_dataset,\n",
    "        }, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(image_test)):\n",
    "    test(i, \"2streak100epochs\", \"task1\", \"softexudates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.46484309434891\n",
      "26.250777304172516\n",
      "20.4270920753479\n",
      "13.255280308425426\n",
      "7.77558471262455\n",
      "4.586254129186273\n",
      "2.3648105040192604\n",
      "1.5346070677042007\n",
      "0.9536549397744238\n",
      "0.5772387200267985\n",
      "13.45454003661871\n",
      "4.412263004109263\n",
      "2.074827627511695\n",
      "1.1346344805788249\n",
      "0.6379011685494334\n",
      "0.3920715559506789\n",
      "0.2104671751440037\n",
      "0.22020873207657132\n",
      "0.1363369211758254\n",
      "0.12140374958107714\n",
      "16.681800670921803\n",
      "4.65898478589952\n",
      "2.5014692340046167\n",
      "1.4243572717532516\n",
      "1.056384357623756\n",
      "0.538886453199666\n",
      "0.393902646668721\n",
      "0.2203140258206986\n",
      "0.11009480235225055\n",
      "0.13226510648382828\n",
      "9.487128367647529\n",
      "3.457006884738803\n",
      "1.8514068583026528\n",
      "1.1478114165365696\n",
      "0.7540193410823122\n",
      "0.9356588879600167\n",
      "0.8494800234329887\n",
      "0.3681835462921299\n",
      "0.26040642510633916\n",
      "0.19696869706967846\n",
      "9.565196793526411\n",
      "3.100829637609422\n",
      "1.839425751240924\n",
      "1.0322476993314922\n",
      "0.6479089525528252\n",
      "0.4886001986451447\n",
      "0.22339871019357815\n",
      "0.21318444173084572\n",
      "0.1461899876157986\n",
      "0.09051430795079796\n"
     ]
    }
   ],
   "source": [
    "learning = []\n",
    "\n",
    "for dataset in losses_per_dataset:\n",
    "    for epoch in dataset:\n",
    "        learning.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learning)\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.savefig('125epochs_1streak.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
